From edf921fb80051b877bc4ce17b139de4b961ba335 Mon Sep 17 00:00:00 2001
From: kknitin <nitin@elk.audio>
Date: Wed, 29 Sep 2021 14:46:00 +0000
Subject: [PATCH] Driver mods for safe rt dma

---
 drivers/dma/bcm2835-dma.c       | 184 ++++++++++++++++++++++++++++----
 drivers/dma/virt-dma.c          |  15 ++-
 drivers/dma/virt-dma.h          |  16 +--
 include/linux/dma/bcm2835-dma.h |  17 +++
 4 files changed, 198 insertions(+), 34 deletions(-)
 create mode 100644 include/linux/dma/bcm2835-dma.h

diff --git a/drivers/dma/bcm2835-dma.c b/drivers/dma/bcm2835-dma.c
index 5a9b18edc2b7..8c0a0bcd52d7 100644
--- a/drivers/dma/bcm2835-dma.c
+++ b/drivers/dma/bcm2835-dma.c
@@ -32,6 +32,8 @@
 #include <linux/spinlock.h>
 #include <linux/of.h>
 #include <linux/of_dma.h>
+#include <rtdm/driver.h>
+#include <linux/ipipe_domain.h>
 
 #include "virt-dma.h"
 
@@ -311,6 +313,13 @@ static const struct bcm2835_dma_cfg_data bcm2711_dma_cfg = {
 	.dma_mask = DMA_BIT_MASK(36),
 };
 
+/* RTDM interrupt objects */
+#define NUM_OF_CBS_FOR_RTDMA 30
+static rtdm_irq_t  dma_rtdm_tx_irq;
+static rtdm_irq_t  dma_rtdm_rx_irq;
+static struct bcm2835_desc *rt_tx_desc = NULL;
+static struct bcm2835_desc *rt_rx_desc = NULL;
+
 static inline size_t bcm2835_dma_max_frame_length(struct bcm2835_chan *c)
 {
 	/* lite and normal channels have different max frame length */
@@ -485,25 +494,35 @@ static struct bcm2835_desc *bcm2835_dma_create_cb_chain(
 	if (!frames)
 		return NULL;
 
-	/* allocate and setup the descriptor. */
-	d = kzalloc(struct_size(d, cb_list, frames), gfp);
-	if (!d)
-		return NULL;
+	if (ipipe_current_domain == ipipe_head_domain) {
+		if (direction == DMA_DEV_TO_MEM)
+			d = rt_rx_desc;
+		else
+			d = rt_tx_desc;
+	} else {
+		/* allocate and setup the descriptor. */
+		d = kzalloc(sizeof(*d) + frames * sizeof(struct bcm2835_cb_entry),
+				gfp);
+		if (!d)
+			return NULL;
+	}
 
 	d->c = c;
 	d->dir = direction;
 	d->cyclic = cyclic;
-
+	d->frames = 0;
 	/*
 	 * Iterate over all frames, create a control block
 	 * for each frame and link them together.
 	 */
 	for (frame = 0, total_len = 0; frame < frames; d->frames++, frame++) {
 		cb_entry = &d->cb_list[frame];
-		cb_entry->cb = dma_pool_alloc(c->cb_pool, gfp,
-					      &cb_entry->paddr);
-		if (!cb_entry->cb)
-			goto error_cb;
+		if (ipipe_current_domain != ipipe_head_domain) {
+			cb_entry->cb = dma_pool_alloc(c->cb_pool, gfp,
+						&cb_entry->paddr);
+			if (!cb_entry->cb)
+				goto error_cb;
+		}
 
 		/* fill in the control block */
 		control_block = cb_entry->cb;
@@ -545,8 +564,9 @@ static struct bcm2835_desc *bcm2835_dma_create_cb_chain(
 			d->cb_list[frame - 1].cb->next = cb_entry->paddr;
 
 		/* update src and dst and length */
-		if (src && (info & BCM2835_DMA_S_INC))
+		if (src && (info & BCM2835_DMA_S_INC)) {
 			src += control_block->length;
+		}
 		if (dst && (info & BCM2835_DMA_D_INC))
 			dst += control_block->length;
 
@@ -682,6 +702,53 @@ static void bcm2835_dma_start_desc(struct bcm2835_chan *c)
 	}
 }
 
+static void bcm2835_chan_complete_cyclic(struct virt_dma_desc *vd)
+{
+	struct virt_dma_chan *vc = to_virt_chan(vd->tx.chan);
+
+	vc->cyclic = vd;
+	vchan_complete((unsigned long)vc);
+}
+
+static void bcm2835_chan_complete(struct virt_dma_desc *vd)
+{
+	struct virt_dma_chan *vc = to_virt_chan(vd->tx.chan);
+	dma_cookie_t cookie;
+
+	cookie = vd->tx.cookie;
+	dma_cookie_complete(&vd->tx);
+	dev_vdbg(vc->chan.device->dev, "txd %p[%x]: marked complete\n",
+		 vd, cookie);
+	list_add_tail(&vd->node, &vc->desc_completed);
+
+	vchan_complete((unsigned long)vc);
+}
+
+static int bcm2835_dma_rtcallback(rtdm_irq_t *irqh)
+{
+	struct bcm2835_chan *c = rtdm_irq_get_arg(irqh, struct bcm2835_chan);
+	struct bcm2835_desc *d;
+	unsigned long flags;
+
+	/* Acknowledge interrupt */
+	writel(BCM2835_DMA_INT | BCM2835_DMA_ACTIVE,
+	       c->chan_base + BCM2835_DMA_CS);
+
+	d = c->desc;
+
+	if (d) {
+		if (d->cyclic) {
+			/* call the cyclic callback */
+			bcm2835_chan_complete_cyclic(&d->vd);
+		} else {
+			bcm2835_chan_complete(&c->desc->vd);
+			bcm2835_dma_start_desc(c);
+		}
+	}
+
+	return RTDM_IRQ_HANDLED;
+}
+
 static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 {
 	struct bcm2835_chan *c = data;
@@ -697,7 +764,7 @@ static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 			return IRQ_NONE;
 	}
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 
 	/*
 	 * Clear the INT flag to receive further interrupts. Keep the channel
@@ -721,7 +788,7 @@ static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 		}
 	}
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 
 	return IRQ_HANDLED;
 }
@@ -755,8 +822,7 @@ static void bcm2835_dma_free_chan_resources(struct dma_chan *chan)
 	vchan_free_chan_resources(&c->vc);
 	free_irq(c->irq_number, c);
 	dma_pool_destroy(c->cb_pool);
-
-	dev_dbg(c->vc.chan.device->dev, "Freeing DMA channel %u\n", c->ch);
+	dev_dbg(c->vc.chan.device->dev, "Freeing DMA channel %u\n",c->ch);
 }
 
 static size_t bcm2835_dma_desc_size(struct bcm2835_desc *d)
@@ -800,7 +866,7 @@ static enum dma_status bcm2835_dma_tx_status(struct dma_chan *chan,
 	if (ret == DMA_COMPLETE || !txstate)
 		return ret;
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 	vd = vchan_find_desc(&c->vc, cookie);
 	if (vd) {
 		txstate->residue =
@@ -829,7 +895,7 @@ static enum dma_status bcm2835_dma_tx_status(struct dma_chan *chan,
 		txstate->residue = 0;
 	}
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 
 	return ret;
 }
@@ -839,11 +905,11 @@ static void bcm2835_dma_issue_pending(struct dma_chan *chan)
 	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
 	unsigned long flags;
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 	if (vchan_issue_pending(&c->vc) && !c->desc)
 		bcm2835_dma_start_desc(c);
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 }
 
 static struct dma_async_tx_descriptor *bcm2835_dma_prep_dma_memcpy(
@@ -1047,7 +1113,7 @@ static int bcm2835_dma_terminate_all(struct dma_chan *chan)
 	unsigned long flags;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 
 	/* stop DMA activity */
 	if (c->desc) {
@@ -1060,7 +1126,7 @@ static int bcm2835_dma_terminate_all(struct dma_chan *chan)
 	}
 
 	vchan_get_all_descriptors(&c->vc, &head);
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 	vchan_dma_desc_free_list(&c->vc, &head);
 
 	return 0;
@@ -1427,6 +1493,84 @@ static void bcm2835_dma_exit(void)
 	platform_driver_unregister(&bcm2835_dma_driver);
 }
 
+int bcm2835_dma_alloc_rtdm_resources(struct dma_chan *chan,
+				enum dma_transfer_direction direction)
+{
+	struct bcm2835_desc *d;
+	struct bcm2835_cb_entry *cb_entry;
+	static rtdm_irq_t *rtdm_irq_handle;
+	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
+	int frame, ret = 0;
+	size_t frames = NUM_OF_CBS_FOR_RTDMA;
+
+	d = kzalloc(sizeof(*d) + frames *
+	sizeof(struct bcm2835_cb_entry),
+			GFP_KERNEL);
+	if (!d)
+		return -ENOMEM;
+	for (frame = 0; frame < frames; frame++) {
+		cb_entry = &d->cb_list[frame];
+		cb_entry->cb = dma_pool_alloc(c->cb_pool, GFP_KERNEL,
+				&cb_entry->paddr);
+		if (!cb_entry->cb) {
+			printk("dma_pool_alloc failed\n");
+			return -ENOMEM;
+		}
+	}
+	if (direction == DMA_MEM_TO_DEV) {
+		rt_tx_desc = d;
+		rtdm_irq_handle = &dma_rtdm_tx_irq;
+	} else if (direction == DMA_DEV_TO_MEM) {
+		rt_rx_desc = d;
+		rtdm_irq_handle = &dma_rtdm_rx_irq;
+	} else {
+		for (frame = 0; frame < frames; frame++) {
+			dma_pool_free(c->cb_pool,
+			d->cb_list[frame].cb,
+			d->cb_list[frame].paddr);
+		}
+		kfree(d);
+		return -1;
+	}
+	/* free the prev non-rt irq */
+	free_irq(c->irq_number, c);
+	ret = rtdm_irq_request(rtdm_irq_handle, c->irq_number,
+				bcm2835_dma_rtcallback,
+				0, "RT DMA IRQ", c);
+	if (ret) {
+		printk(KERN_ERR "bcm2835-dma: RT IRQ request failed\n");
+	}
+	return ret;
+}
+EXPORT_SYMBOL_GPL(bcm2835_dma_alloc_rtdm_resources);
+
+int bcm2835_dma_free_rtdm_resources(struct dma_chan *chan,
+				enum dma_transfer_direction direction)
+{
+	int frame;
+	struct bcm2835_desc *d;
+	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
+	size_t frames = NUM_OF_CBS_FOR_RTDMA;
+	if (direction == DMA_MEM_TO_DEV) {
+		d = rt_tx_desc;
+		rtdm_irq_free(&dma_rtdm_tx_irq);
+	} else if (direction == DMA_DEV_TO_MEM) {
+		d = rt_rx_desc;
+		rtdm_irq_free(&dma_rtdm_rx_irq);
+	} else {
+		return -1;
+	}
+	for (frame = 0; frame < frames; frame++) {
+		dma_pool_free(c->cb_pool,
+		d->cb_list[frame].cb,
+		d->cb_list[frame].paddr);
+	}
+	kfree(d);
+	return request_irq(c->irq_number, bcm2835_dma_callback,
+			   c->irq_flags, "DMA IRQ", c);
+}
+EXPORT_SYMBOL_GPL(bcm2835_dma_free_rtdm_resources);
+
 /*
  * Load after serial driver (arch_initcall) so we see the messages if it fails,
  * but before drivers (module_init) that need a DMA channel.
diff --git a/drivers/dma/virt-dma.c b/drivers/dma/virt-dma.c
index 256fc662c500..db20d86d534b 100644
--- a/drivers/dma/virt-dma.c
+++ b/drivers/dma/virt-dma.c
@@ -23,11 +23,11 @@ dma_cookie_t vchan_tx_submit(struct dma_async_tx_descriptor *tx)
 	unsigned long flags;
 	dma_cookie_t cookie;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	cookie = dma_cookie_assign(tx);
 
 	list_move_tail(&vd->node, &vc->desc_submitted);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	dev_dbg(vc->chan.device->dev, "vchan %p: txd %p[%x]: submitted\n",
 		vc, vd, cookie);
@@ -52,9 +52,9 @@ int vchan_tx_desc_free(struct dma_async_tx_descriptor *tx)
 	struct virt_dma_desc *vd = to_virt_desc(tx);
 	unsigned long flags;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	list_del(&vd->node);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	dev_dbg(vc->chan.device->dev, "vchan %p: txd %p[%x]: freeing\n",
 		vc, vd, vd->tx.cookie);
@@ -80,14 +80,13 @@ EXPORT_SYMBOL_GPL(vchan_find_desc);
  * This tasklet handles the completion of a DMA descriptor by
  * calling its callback and freeing it.
  */
-static void vchan_complete(unsigned long arg)
+void vchan_complete(unsigned long arg)
 {
 	struct virt_dma_chan *vc = (struct virt_dma_chan *)arg;
 	struct virt_dma_desc *vd, *_vd;
 	struct dmaengine_desc_callback cb;
 	LIST_HEAD(head);
 
-	spin_lock_irq(&vc->lock);
 	list_splice_tail_init(&vc->desc_completed, &head);
 	vd = vc->cyclic;
 	if (vd) {
@@ -96,7 +95,6 @@ static void vchan_complete(unsigned long arg)
 	} else {
 		memset(&cb, 0, sizeof(cb));
 	}
-	spin_unlock_irq(&vc->lock);
 
 	dmaengine_desc_callback_invoke(&cb, &vd->tx_result);
 
@@ -108,6 +106,7 @@ static void vchan_complete(unsigned long arg)
 		vchan_vdesc_fini(vd);
 	}
 }
+EXPORT_SYMBOL_GPL(vchan_complete);
 
 void vchan_dma_desc_free_list(struct virt_dma_chan *vc, struct list_head *head)
 {
@@ -129,7 +128,7 @@ void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev)
 {
 	dma_cookie_init(&vc->chan);
 
-	spin_lock_init(&vc->lock);
+	raw_spin_lock_init(&vc->lock);
 	INIT_LIST_HEAD(&vc->desc_allocated);
 	INIT_LIST_HEAD(&vc->desc_submitted);
 	INIT_LIST_HEAD(&vc->desc_issued);
diff --git a/drivers/dma/virt-dma.h b/drivers/dma/virt-dma.h
index ab158bac03a7..a35deb9d81be 100644
--- a/drivers/dma/virt-dma.h
+++ b/drivers/dma/virt-dma.h
@@ -24,8 +24,11 @@ struct virt_dma_chan {
 	struct tasklet_struct task;
 	void (*desc_free)(struct virt_dma_desc *);
 
-	spinlock_t lock;
-
+#ifdef CONFIG_IPIPE
+	ipipe_spinlock_t lock;
+#else
+	raw_spinlock_t lock;
+#endif
 	/* protected by vc.lock */
 	struct list_head desc_allocated;
 	struct list_head desc_submitted;
@@ -46,6 +49,7 @@ void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev);
 struct virt_dma_desc *vchan_find_desc(struct virt_dma_chan *, dma_cookie_t);
 extern dma_cookie_t vchan_tx_submit(struct dma_async_tx_descriptor *);
 extern int vchan_tx_desc_free(struct dma_async_tx_descriptor *);
+extern void vchan_complete(unsigned long arg);
 
 /**
  * vchan_tx_prep - prepare a descriptor
@@ -66,9 +70,9 @@ static inline struct dma_async_tx_descriptor *vchan_tx_prep(struct virt_dma_chan
 	vd->tx_result.result = DMA_TRANS_NOERROR;
 	vd->tx_result.residue = 0;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	list_add_tail(&vd->node, &vc->desc_allocated);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	return &vd->tx;
 }
@@ -187,11 +191,11 @@ static inline void vchan_free_chan_resources(struct virt_dma_chan *vc)
 	unsigned long flags;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	vchan_get_all_descriptors(vc, &head);
 	list_for_each_entry(vd, &head, node)
 		dmaengine_desc_clear_reuse(&vd->tx);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	vchan_dma_desc_free_list(vc, &head);
 }
diff --git a/include/linux/dma/bcm2835-dma.h b/include/linux/dma/bcm2835-dma.h
new file mode 100644
index 000000000000..69256906c29c
--- /dev/null
+++ b/include/linux/dma/bcm2835-dma.h
@@ -0,0 +1,17 @@
+/*
+* BCM2835 DMA exports for RT safe dma for Elk Audio OS.
+* @copyright 2017-2020 Modern Ancient Instruments Networked AB, dba Elk,
+* Stockholm
+*/
+#ifndef _BCM2835_DMA_H
+#define _BCM2835_DMA_H
+
+#include <linux/dmaengine.h>
+
+int bcm2835_dma_alloc_rtdm_resources(struct dma_chan *chan,
+				enum dma_transfer_direction direction);
+
+int bcm2835_dma_free_rtdm_resources(struct dma_chan *chan,
+				enum dma_transfer_direction direction);
+
+#endif
-- 
2.17.1

