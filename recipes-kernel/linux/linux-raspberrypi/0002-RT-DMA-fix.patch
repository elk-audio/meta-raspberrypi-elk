From 9eda28bc0c9c5834c262f4fcd8c99220fb63fb01 Mon Sep 17 00:00:00 2001
From: kknitin <nitin@elk.audio>
Date: Mon, 20 Jan 2020 23:05:34 +0000
Subject: [PATCH] RT DMA fix

---
 drivers/dma/bcm2835-dma.c | 124 +++++++++++++++++++++++++++++++++-----
 drivers/dma/virt-dma.c    |  15 +++--
 drivers/dma/virt-dma.h    |  16 +++--
 3 files changed, 126 insertions(+), 29 deletions(-)

diff --git a/drivers/dma/bcm2835-dma.c b/drivers/dma/bcm2835-dma.c
index 4183734a1fbf..1427980082b2 100644
--- a/drivers/dma/bcm2835-dma.c
+++ b/drivers/dma/bcm2835-dma.c
@@ -44,6 +44,7 @@
 #include <linux/spinlock.h>
 #include <linux/of.h>
 #include <linux/of_dma.h>
+#include <rtdm/driver.h>
 
 #include "virt-dma.h"
 
@@ -308,6 +309,10 @@ static const struct bcm2835_dma_cfg_data bcm2838_dma_cfg = {
 	.chan_40bit_mask = BIT(11) | BIT(12) | BIT(13) | BIT(14),
 };
 
+/* RTDM interrupt objects */
+static rtdm_irq_t  dma_rtdm_tx_irq;
+static rtdm_irq_t  dma_rtdm_rx_irq;
+
 static inline size_t bcm2835_dma_max_frame_length(struct bcm2835_chan *c)
 {
 	/* lite and normal channels have different max frame length */
@@ -542,8 +547,9 @@ static struct bcm2835_desc *bcm2835_dma_create_cb_chain(
 			d->cb_list[frame - 1].cb->next = cb_entry->paddr;
 
 		/* update src and dst and length */
-		if (src && (info & BCM2835_DMA_S_INC))
+		if (src && (info & BCM2835_DMA_S_INC)) {
 			src += control_block->length;
+		}
 		if (dst && (info & BCM2835_DMA_D_INC))
 			dst += control_block->length;
 
@@ -676,6 +682,56 @@ static void bcm2835_dma_start_desc(struct bcm2835_chan *c)
 	}
 }
 
+static void bcm2835_chan_complete_cyclic(struct virt_dma_desc *vd)
+{
+	struct virt_dma_chan *vc = to_virt_chan(vd->tx.chan);
+
+	vc->cyclic = vd;
+	vchan_complete((unsigned long)vc);
+}
+
+static void bcm2835_chan_complete(struct virt_dma_desc *vd)
+{
+	struct virt_dma_chan *vc = to_virt_chan(vd->tx.chan);
+	dma_cookie_t cookie;
+
+	cookie = vd->tx.cookie;
+	dma_cookie_complete(&vd->tx);
+	dev_vdbg(vc->chan.device->dev, "txd %p[%x]: marked complete\n",
+		 vd, cookie);
+	list_add_tail(&vd->node, &vc->desc_completed);
+
+	vchan_complete((unsigned long)vc);
+}
+
+static int bcm2835_dma_rtcallback(rtdm_irq_t *irqh)
+{
+	struct bcm2835_chan *c = rtdm_irq_get_arg(irqh, struct bcm2835_chan);
+	struct bcm2835_desc *d;
+	unsigned long flags;
+
+	/* Acknowledge interrupt */
+	writel(BCM2835_DMA_INT, c->chan_base + BCM2835_DMA_CS);
+
+	d = c->desc;
+
+	if (d) {
+		if (d->cyclic) {
+			/* call the cyclic callback */
+			bcm2835_chan_complete_cyclic(&d->vd);
+
+			/* Keep the DMA engine running */
+			writel(BCM2835_DMA_ACTIVE,
+			       c->chan_base + BCM2835_DMA_CS);
+		} else {
+			bcm2835_chan_complete(&c->desc->vd);
+			bcm2835_dma_start_desc(c);
+		}
+	}
+
+	return RTDM_IRQ_HANDLED;
+}
+
 static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 {
 	struct bcm2835_chan *c = data;
@@ -691,7 +747,7 @@ static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 			return IRQ_NONE;
 	}
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 
 	/*
 	 * Clear the INT flag to receive further interrupts. Keep the channel
@@ -717,7 +773,7 @@ static irqreturn_t bcm2835_dma_callback(int irq, void *data)
 		}
 	}
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 
 	return IRQ_HANDLED;
 }
@@ -740,15 +796,52 @@ static int bcm2835_dma_alloc_chan_resources(struct dma_chan *chan)
 			   c->irq_flags, "DMA IRQ", c);
 }
 
+/* This handler was not present for bcm2835-dma and most certainly no one calls
+it. Hence, it's safe to use this to enter dma backend to get the rtdm irqs */
+static int bcm2835_dma_resume(struct dma_chan *chan)
+{
+	static rtdm_irq_t *rtdm_irq_handle;
+	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
+	int ret = 0;
+
+	if (chan->private) {
+		if (!strcmp(chan->private, "rtdm-tx-irq")) {
+			rtdm_irq_handle = &dma_rtdm_tx_irq;
+		} else if (!strcmp(chan->private, "rtdm-rx-irq")) {
+			rtdm_irq_handle = &dma_rtdm_rx_irq;
+		} else {
+			return ret;
+		}
+		free_irq(c->irq_number, c);
+		ret = rtdm_irq_request(rtdm_irq_handle, c->irq_number,
+					bcm2835_dma_rtcallback,
+					0, "RT DMA IRQ", c);
+		if (ret) {
+			printk(KERN_ERR "bcm2835-dma: RT IRQ request failed\n");
+		}
+	}
+	return ret;
+}
+
 static void bcm2835_dma_free_chan_resources(struct dma_chan *chan)
 {
 	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
 
 	vchan_free_chan_resources(&c->vc);
-	free_irq(c->irq_number, c);
-	dma_pool_destroy(c->cb_pool);
 
-	dev_dbg(c->vc.chan.device->dev, "Freeing DMA channel %u\n", c->ch);
+	if (chan->private) {
+		if (!strcmp(chan->private, "rtdm-tx-irq")) {
+			rtdm_irq_free(&dma_rtdm_tx_irq);
+			goto exit;
+		} else if (!strcmp(chan->private, "rtdm-rx-irq")) {
+			rtdm_irq_free(&dma_rtdm_rx_irq);
+			goto exit;
+		}
+	}
+
+	free_irq(c->irq_number, c);
+exit:	dma_pool_destroy(c->cb_pool);
+	dev_dbg(c->vc.chan.device->dev, "Freeing DMA channel %u\n",c->ch);
 }
 
 static size_t bcm2835_dma_desc_size(struct bcm2835_desc *d)
@@ -792,7 +885,7 @@ static enum dma_status bcm2835_dma_tx_status(struct dma_chan *chan,
 	if (ret == DMA_COMPLETE || !txstate)
 		return ret;
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 	vd = vchan_find_desc(&c->vc, cookie);
 	if (vd) {
 		txstate->residue =
@@ -821,7 +914,7 @@ static enum dma_status bcm2835_dma_tx_status(struct dma_chan *chan,
 		txstate->residue = 0;
 	}
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 
 	return ret;
 }
@@ -831,11 +924,11 @@ static void bcm2835_dma_issue_pending(struct dma_chan *chan)
 	struct bcm2835_chan *c = to_bcm2835_dma_chan(chan);
 	unsigned long flags;
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 	if (vchan_issue_pending(&c->vc) && !c->desc)
 		bcm2835_dma_start_desc(c);
 
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 }
 
 static struct dma_async_tx_descriptor *bcm2835_dma_prep_dma_memcpy(
@@ -1030,12 +1123,12 @@ static int bcm2835_dma_terminate_all(struct dma_chan *chan)
 	unsigned long flags;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&c->vc.lock, flags);
+	raw_spin_lock_irqsave(&c->vc.lock, flags);
 
 	/* Prevent this channel being scheduled */
-	spin_lock(&d->lock);
+	raw_spin_lock(&d->lock);
 	list_del_init(&c->node);
-	spin_unlock(&d->lock);
+	raw_spin_unlock(&d->lock);
 
 	/* stop DMA activity */
 	if (c->desc) {
@@ -1045,7 +1138,7 @@ static int bcm2835_dma_terminate_all(struct dma_chan *chan)
 	}
 
 	vchan_get_all_descriptors(&c->vc, &head);
-	spin_unlock_irqrestore(&c->vc.lock, flags);
+	raw_spin_unlock_irqrestore(&c->vc.lock, flags);
 	vchan_dma_desc_free_list(&c->vc, &head);
 
 	return 0;
@@ -1216,6 +1309,7 @@ static int bcm2835_dma_probe(struct platform_device *pdev)
 	dma_cap_set(DMA_CYCLIC, od->ddev.cap_mask);
 	dma_cap_set(DMA_SLAVE, od->ddev.cap_mask);
 	dma_cap_set(DMA_MEMCPY, od->ddev.cap_mask);
+	od->ddev.device_resume = bcm2835_dma_resume;
 	od->ddev.device_alloc_chan_resources = bcm2835_dma_alloc_chan_resources;
 	od->ddev.device_free_chan_resources = bcm2835_dma_free_chan_resources;
 	od->ddev.device_tx_status = bcm2835_dma_tx_status;
@@ -1233,7 +1327,7 @@ static int bcm2835_dma_probe(struct platform_device *pdev)
 	od->ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;
 	od->ddev.dev = &pdev->dev;
 	INIT_LIST_HEAD(&od->ddev.channels);
-	spin_lock_init(&od->lock);
+	raw_spin_lock_init(&od->lock);
 
 	platform_set_drvdata(pdev, od);
 
diff --git a/drivers/dma/virt-dma.c b/drivers/dma/virt-dma.c
index 88ad8ed2a8d6..594749fd1941 100644
--- a/drivers/dma/virt-dma.c
+++ b/drivers/dma/virt-dma.c
@@ -26,11 +26,11 @@ dma_cookie_t vchan_tx_submit(struct dma_async_tx_descriptor *tx)
 	unsigned long flags;
 	dma_cookie_t cookie;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	cookie = dma_cookie_assign(tx);
 
 	list_move_tail(&vd->node, &vc->desc_submitted);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	dev_dbg(vc->chan.device->dev, "vchan %p: txd %p[%x]: submitted\n",
 		vc, vd, cookie);
@@ -55,9 +55,9 @@ int vchan_tx_desc_free(struct dma_async_tx_descriptor *tx)
 	struct virt_dma_desc *vd = to_virt_desc(tx);
 	unsigned long flags;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	list_del(&vd->node);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	dev_dbg(vc->chan.device->dev, "vchan %p: txd %p[%x]: freeing\n",
 		vc, vd, vd->tx.cookie);
@@ -83,14 +83,13 @@ EXPORT_SYMBOL_GPL(vchan_find_desc);
  * This tasklet handles the completion of a DMA descriptor by
  * calling its callback and freeing it.
  */
-static void vchan_complete(unsigned long arg)
+void vchan_complete(unsigned long arg)
 {
 	struct virt_dma_chan *vc = (struct virt_dma_chan *)arg;
 	struct virt_dma_desc *vd, *_vd;
 	struct dmaengine_desc_callback cb;
 	LIST_HEAD(head);
 
-	spin_lock_irq(&vc->lock);
 	list_splice_tail_init(&vc->desc_completed, &head);
 	vd = vc->cyclic;
 	if (vd) {
@@ -99,7 +98,6 @@ static void vchan_complete(unsigned long arg)
 	} else {
 		memset(&cb, 0, sizeof(cb));
 	}
-	spin_unlock_irq(&vc->lock);
 
 	dmaengine_desc_callback_invoke(&cb, NULL);
 
@@ -112,6 +110,7 @@ static void vchan_complete(unsigned long arg)
 		dmaengine_desc_callback_invoke(&cb, NULL);
 	}
 }
+EXPORT_SYMBOL_GPL(vchan_complete);
 
 void vchan_dma_desc_free_list(struct virt_dma_chan *vc, struct list_head *head)
 {
@@ -133,7 +132,7 @@ void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev)
 {
 	dma_cookie_init(&vc->chan);
 
-	spin_lock_init(&vc->lock);
+	raw_spin_lock_init(&vc->lock);
 	INIT_LIST_HEAD(&vc->desc_allocated);
 	INIT_LIST_HEAD(&vc->desc_submitted);
 	INIT_LIST_HEAD(&vc->desc_issued);
diff --git a/drivers/dma/virt-dma.h b/drivers/dma/virt-dma.h
index b09b75ab0751..d3564a479467 100644
--- a/drivers/dma/virt-dma.h
+++ b/drivers/dma/virt-dma.h
@@ -26,8 +26,11 @@ struct virt_dma_chan {
 	struct tasklet_struct task;
 	void (*desc_free)(struct virt_dma_desc *);
 
-	spinlock_t lock;
-
+#ifdef CONFIG_IPIPE
+	ipipe_spinlock_t lock;
+#else
+	raw_spinlock_t lock;
+#endif
 	/* protected by vc.lock */
 	struct list_head desc_allocated;
 	struct list_head desc_submitted;
@@ -48,6 +51,7 @@ void vchan_init(struct virt_dma_chan *vc, struct dma_device *dmadev);
 struct virt_dma_desc *vchan_find_desc(struct virt_dma_chan *, dma_cookie_t);
 extern dma_cookie_t vchan_tx_submit(struct dma_async_tx_descriptor *);
 extern int vchan_tx_desc_free(struct dma_async_tx_descriptor *);
+extern void vchan_complete(unsigned long arg);
 
 /**
  * vchan_tx_prep - prepare a descriptor
@@ -65,9 +69,9 @@ static inline struct dma_async_tx_descriptor *vchan_tx_prep(struct virt_dma_chan
 	vd->tx.tx_submit = vchan_tx_submit;
 	vd->tx.desc_free = vchan_tx_desc_free;
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	list_add_tail(&vd->node, &vc->desc_allocated);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	return &vd->tx;
 }
@@ -186,11 +190,11 @@ static inline void vchan_free_chan_resources(struct virt_dma_chan *vc)
 	unsigned long flags;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&vc->lock, flags);
+	raw_spin_lock_irqsave(&vc->lock, flags);
 	vchan_get_all_descriptors(vc, &head);
 	list_for_each_entry(vd, &head, node)
 		dmaengine_desc_clear_reuse(&vd->tx);
-	spin_unlock_irqrestore(&vc->lock, flags);
+	raw_spin_unlock_irqrestore(&vc->lock, flags);
 
 	vchan_dma_desc_free_list(vc, &head);
 }
-- 
2.17.1

